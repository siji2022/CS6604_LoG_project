{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae6327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (1.51.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/sijic/.local/lib/python3.9/site-packages (from openai) (4.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/sijic/anaconda3/envs/mts/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de226f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b40ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is my API key, you should use your own\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "def get_chat_completion(messages,client=client, model=\"gpt-3.5\"):\n",
    "    # the cheapest model I could find; input $0.0005 / 1K tokens; output $0.0015 / 1K tokens\n",
    "    chat_completion= client.chat.completions.create(\n",
    "        messages=[\n",
    "             {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": messages,\n",
    "        }\n",
    "            ],\n",
    "        model=model,\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_completion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd673f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response=get_chat_completion(\"What is the capital of France?\",client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f0ddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AIgPxQF9j6y7POSscTIm7ZHllkLeE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1729016321, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62295a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a3c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 21 prompt tokens: 14 completion_tokens tokens: 7\n"
     ]
    }
   ],
   "source": [
    "# print the tokens size\n",
    "print('total tokens:', response.usage.total_tokens, 'prompt tokens:', response.usage.prompt_tokens, 'completion_tokens tokens:', response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9a8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(response, debug=False):\n",
    "    if debug:\n",
    "        # print the tokens size\n",
    "        print('total tokens:', response.usage.total_tokens, 'prompt tokens:', response.usage.prompt_tokens, 'completion_tokens tokens:', response.usage.completion_tokens)\n",
    "    print('response:', response.choices[0].message.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24132a0",
   "metadata": {},
   "source": [
    "## Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5449cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_review = \"\"\"\n",
    "Stock investment recommendation is crucial for guiding investment decisions and managing portfolios. Recent studies have demonstrated the potential of temporal-relational models (TRM) to yield excess investment returns. However, in the complicated finance ecosystem, the current TRM suffer from both the intrinsic temporal bias from the low signal-to-noise ratio (SNR) and the relational bias caused by utilizing inappropriate relational topologies and propagation mechanisms. Moreover, the distribution shifts behind macro-market scenarios invalidate the underlying i.i.d. assumption and limit the generalization ability of TRM. In this paper, we pioneer the impact of the above issues on the effective learning of temporal-relational patterns and propose an Automatic De-Biased Temporal-Relational Model (ADB-TRM) for stock recommendation. Specifically, ADB-TRM consists of three main components, i.e., (i) a meta-learned architecture forms a dual-stage training process, with the inner part ameliorating temporal-relational bias and the outer meta-learner counteracting distribution shifts,  (ii) automatic adversarial sample generation guides the model adaptively to alleviate bias and enhance its profiling ability through adversarial training, and (iii) global-local interaction helps seek relative invariant stock embeddings from local and global distribution perspectives to mitigate distribution shifts. Experiments on three datasets from distinct stock markets show that ADB-TRM excels state-of-the-arts over 28.41% and 9.53% in terms of cumulative and risk-adjusted returns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96cee814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 445 prompt tokens: 379 completion_tokens tokens: 66\n",
      "response: Motivation: Improve stock investment recommendation.\n",
      "Previous solution limitation: Temporal and relational bias, distribution shifts.\n",
      "Proposed solution: Automatic De-Biased Temporal-Relational Model (ADB-TRM).\n",
      "Comparison: ADB-TRM outperforms previous solution by 28.41% and 9.53%.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Answer the following questions after read the paper abstract. Give me concise and deterministic results. Each answer should be less than 10 words.\n",
    "Paper abstract: ```{prod_review}```\n",
    "What is the motivation of the research?\n",
    "What is the previous best solution limited to?\n",
    "What is the proposed solution?\n",
    "How does the proposed solution compare to the previous best solution?\n",
    "\"\"\"\n",
    "\n",
    "response = get_chat_completion(prompt)\n",
    "print_response(response, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b60d9504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 445 prompt tokens: 377 completion_tokens tokens: 68\n",
      "response: 1. Improve stock investment recommendations and portfolio management.  \n",
      "2. Intrinsic temporal and relational biases, generalization issues.  \n",
      "3. Automatic De-Biased Temporal-Relational Model (ADB-TRM).  \n",
      "4. ADB-TRM outperforms previous solutions by 28.41% and 9.53%.\n"
     ]
    }
   ],
   "source": [
    "response = get_chat_completion(prompt, model=\"gpt-4o-mini\")\n",
    "print_response(response, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e44dbe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AIgZ69F5hP1NaWGdnveTAqH1uyy1L', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The customer loved the soft and cute panda plush toy but found it a bit small for the price paid. However, they were impressed with the early delivery.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1729016888, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=172, total_tokens=203, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to generate a short summary of a product \\\n",
    "review from an ecommerce site to give feedback to the \\\n",
    "Shipping deparmtment. \n",
    "\n",
    "Summarize the review below, delimited by triple \n",
    "backticks, in at most 30 words, and focusing on any aspects \\\n",
    "that mention shipping and delivery of the product. \n",
    "\n",
    "Review: ```{prod_review}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_chat_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e749a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 203 prompt tokens: 172 completion_tokens tokens: 31\n",
      "response: The customer loved the soft and cute panda plush toy but found it a bit small for the price paid. However, they were impressed with the early delivery.\n"
     ]
    }
   ],
   "source": [
    "print_response(response, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5397438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 196 prompt tokens: 161 completion_tokens tokens: 35\n",
      "response: Feedback: The item arrived a day earlier than expected, allowing the customer to enjoy it before gifting it. Consider offering larger options for the same price to enhance customer satisfaction.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to extract relevant information from \\ \n",
    "a product review from an ecommerce site to give \\\n",
    "feedback to the Shipping department. \n",
    "\n",
    "From the review below, delimited by triple quotes \\\n",
    "extract the information relevant to shipping and \\ \n",
    "delivery. Limit to 30 words. \n",
    "\n",
    "Review: ```{prod_review}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_chat_completion(prompt)\n",
    "print_response(response, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624ea4d",
   "metadata": {},
   "source": [
    " ### chatbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92ac900",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='gpt-4o-mini'\n",
    "def get_completion(prompt, model=MODEL):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content, response\n",
    "\n",
    "def get_completion_from_messages(messages, model=MODEL, temperature=0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "#     print(str(response.choices[0].message))\n",
    "    return response.choices[0].message.content, response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03ed994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
    "{'role':'user', 'content':'tell me a joke'},   \n",
    "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
    "{'role':'user', 'content':'I don\\'t know'}  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "972379ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To reach the other side, good sir!\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36bb8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Isa! It’s great to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Hi, my name is Isa'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cf878e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hi Isa! It’s great to meet you. How can I help you today?',\n",
       " ChatCompletion(id='chatcmpl-AIh0m2xPmgwyLGB0aRFVBJscP9wRl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hi Isa! It’s great to meet you. How can I help you today?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1729018604, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_e2bde53e6e', usage=CompletionUsage(completion_tokens=17, prompt_tokens=22, total_tokens=39, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249c4c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I don't have access to personal information about individuals unless it has been shared with me during our conversation. If you tell me your name, I can remember it for the duration of our chat!\", ChatCompletion(id='chatcmpl-ALBu9SjboUAL6pirDCO7b6ZzOKQZh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I don't have access to personal information about individuals unless it has been shared with me during our conversation. If you tell me your name, I can remember it for the duration of our chat!\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1729614013, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_482c22a7bc', usage=CompletionUsage(completion_tokens=38, prompt_tokens=20, total_tokens=58, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))))\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "# {'role':'system', 'content':'You are friendly fchatbot.'},    \n",
    "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
